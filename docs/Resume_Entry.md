# 简历项目经历 (Resume Entry) - 智能桌宠项目 (Final)

---

### 项目名称：PyQt5与端侧大模型驱动的智能桌面伴侣 (Smart AI Desktop Companion)
**角色**：独立开发者 / 架构师
**时间**：2026.02 - 2026.05 (示例时间)

#### 项目背景
为了探索 AIGC 技术在端侧交互场景的落地应用，设计并开发了一款集“视觉陪伴”、“情感对话”与“个性化语音”于一体的智能桌面系统。该项目旨在解决传统桌面助手交互僵硬、缺乏情感连接的痛点，实现在普通 PC 上流畅运行高拟真虚拟角色。

#### 核心贡献与技术突破 (STAR)

*   **架构设计 (Architecture & Performance)**:
    *   **双进程异步架构**: 针对 Python GIL 锁导致的 UI 阻塞痛点，设计了 **UI 渲染 (PyQt5)** 与 **AI 推理 (PyTorch)** 分离的双进程架构。通过共享内存 (Shared Memory) 与 IPC 通信，实现了重度计算下的 UI **零卡顿** (稳定 60FPS)。
    *   **极致轻量化渲染**: 基于 Win32 API 深度定制 PyQt5 窗口系统，实现了像素级透明背景与点击穿透算法。在 Idle 状态下 CPU 占用率优化至 **<2%**，内存占用控制在 100MB 以内。

*   **AI 全链路集成 (AI Integration)**:
    *   **大模型对话引擎**: 本地部署并量化微调 **ChatGLM3 (Int4)** 模型，结合 **RAG (检索增强生成)** 技术，赋予了角色长期记忆能力，使其能根据历史对话调整回复策略。
    *   **情感化语音合成**: 接入并优化 **VITS** 语音合成模型，实现了特定角色（如胡桃）的音色克隆。开发了 **流式音频播放 (Streaming Audio)** 模块，将 TTS 首字延迟降低至 200ms 以内，极大提升了对话实时性。
    *   **唇形同步算法 (Lip-Sync)**: 创新性地实现了基于音素 (Phoneme) 驱动的口型同步算法，使角色嘴型能随语音内容精准开合。

*   **工程化与可扩展性 (Engineering)**:
    *   **配置驱动资源系统**: 设计了基于 JSON 的动态资源加载器与 **角色工坊 (Workshop)** 模块，支持用户一键导入 Live2D 模型或 Sprite 序列帧，实现了高度的个性化定制 (UGC)。
    *   **完整开发流**: 输出了包含技术白皮书、API 文档在内的全套工程文档，并遵循敏捷开发流程进行版本迭代与单元测试。

#### 技术栈
*   **Core**: Python 3.9, PyQt5 (GUI), Multiprocessing (并发)
*   **AI Models**: PyTorch, ChatGLM/Qwen (LLM), VITS (TTS), FAISS (Vector DB)
*   **Tools**: Git, PyInstaller, SQLite
*   **Skills**: 端侧模型量化, 异步编程, 内存优化, GUI 渲染原理

---

### 深度面试题准备 (Deep Dive Q&A)

**Q: 如何在本地流畅运行大模型且不影响电脑日常使用？**
**A:** 这是一个核心挑战。我采取了三层优化：
1.  **模型层面**: 使用 Int4 量化技术，将显存需求从 12G 降至 4G 以下。
2.  **调度层面**: 甚至在检测到用户运行大型游戏时，自动挂起 AI 进程，释放资源。
3.  **架构层面**: 彻底的进程隔离，UI 进程极轻量，即使 AI 进程满载计算，桌面角色也不会卡顿。

**Q: 语音合成的延迟是如何优化的？**
**A:** 传统的 TTS 是“生成全部音频 -> 下载 -> 播放”，延迟很高。我采用了**流式 (Streaming)** 方案：AI 模型生成第一个音频块 (Chunk) 时，播放器立即开始播放，同时后台继续生成后续内容。这就像在线看视频一样，不需要等全片下载完。配合 VITS 的快速推理特性，实现了几乎“即问即答”的体验。